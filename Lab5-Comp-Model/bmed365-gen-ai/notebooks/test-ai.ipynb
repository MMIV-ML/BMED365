{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Jupyter AI (for BMED365)\n",
    "\n",
    "Version 2024-01-19 (using the `bmed365ai` environment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter AI\n",
    "\n",
    "https://github.com/jupyterlab/jupyter-ai\n",
    "\n",
    "Welcome to Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:\n",
    "\n",
    "- An `%%ai` magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.).\n",
    "- A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.\n",
    "- Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).\n",
    "\n",
    "Documentation is available on [ReadTheDocs](https://jupyter-ai.readthedocs.io/en/latest).\n",
    "\n",
    "```bash\n",
    "\n",
    "$ conda create -n bmed365ai python=3.11\n",
    "$ conda activate bmed365ai\n",
    "$ pip install jupyter_ai\n",
    "$ pip install openai\n",
    "OPENAI_API_KEY=your-api-key-here jupyter lab\n",
    "# https://platform.openai.com/account/api-keys\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-text-express-v1`, `bedrock:ai21.j2-ultra-v1`, `bedrock:ai21.j2-mid-v1`, `bedrock:cohere.command-light-text-v14`, `bedrock:cohere.command-text-v14`, `bedrock:meta.llama2-13b-chat-v1`, `bedrock:meta.llama2-70b-chat-v1` |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock-chat:anthropic.claude-v1`, `bedrock-chat:anthropic.claude-v2`, `bedrock-chat:anthropic.claude-v2:1`, `bedrock-chat:anthropic.claude-instant-v1` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-2`, `anthropic:claude-2.0`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0`, `anthropic:claude-instant-v1.2` |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic-chat:claude-v1`, `anthropic-chat:claude-v1.0`, `anthropic-chat:claude-v1.2`, `anthropic-chat:claude-2`, `anthropic-chat:claude-2.0`, `anthropic-chat:claude-instant-v1`, `anthropic-chat:claude-instant-v1.0`, `anthropic-chat:claude-instant-v1.2` |\n",
       "| `azure-chat-openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:command`, `cohere:command-nightly`, `cohere:command-light`, `cohere:command-light-nightly` |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `gpt4all:ggml-gpt4all-j-v1.2-jazzy`, `gpt4all:ggml-gpt4all-j-v1.3-groovy`, `gpt4all:ggml-gpt4all-l13b-snoozy`, `gpt4all:mistral-7b-openorca.Q4_0`, `gpt4all:mistral-7b-instruct-v0.1.Q4_0`, `gpt4all:gpt4all-falcon-q4_0`, `gpt4all:wizardlm-13b-v1.2.Q4_0`, `gpt4all:nous-hermes-llama2-13b.Q4_0`, `gpt4all:gpt4all-13b-snoozy-q4_0`, `gpt4all:mpt-7b-chat-merges-q4_0`, `gpt4all:orca-mini-3b-gguf2-q4_0`, `gpt4all:starcoder-q4_0`, `gpt4all:rift-coder-v0-7b-q4_0`, `gpt4all:em_german_mistral_v01.Q4_0` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-16k`, `openai-chat:gpt-3.5-turbo-0301`, `openai-chat:gpt-3.5-turbo-0613`, `openai-chat:gpt-3.5-turbo-16k-0613`, `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-0613`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-4-32k-0613`, `openai-chat:gpt-4-1106-preview` |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | `qianfan:ERNIE-Bot`, `qianfan:ERNIE-Bot-4` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:cohere.command-light-text-v14\n",
       "* bedrock:cohere.command-text-v14\n",
       "* bedrock:meta.llama2-13b-chat-v1\n",
       "* bedrock:meta.llama2-70b-chat-v1\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:anthropic.claude-v1\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-v2:1\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "\n",
       "anthropic\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-2\n",
       "* anthropic:claude-2.0\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "* anthropic:claude-instant-v1.2\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic-chat:claude-v1\n",
       "* anthropic-chat:claude-v1.0\n",
       "* anthropic-chat:claude-v1.2\n",
       "* anthropic-chat:claude-2\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-instant-v1\n",
       "* anthropic-chat:claude-instant-v1.0\n",
       "* anthropic-chat:claude-instant-v1.2\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable: COHERE_API_KEY (not set)\n",
       "* cohere:command\n",
       "* cohere:command-nightly\n",
       "* cohere:command-light\n",
       "* cohere:command-light-nightly\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not recommended:\n",
    "```bash\n",
    "%env OPENAI_API_KEY=sk-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use environment variables to keep [your OpenAI API key](https://platform.openai.com/api-keys) secret. Here's how you can do it:\n",
    "\n",
    "1. Set the environment variable in your system. The way to do this depends on your operating system:\n",
    "\n",
    "   - On Unix systems (like Linux or MacOS), you can use the `export` command in your terminal:\n",
    "\n",
    "     ```bash\n",
    "     export OPENAI_API_KEY='your_api_key_here'\n",
    "     ```\n",
    "\n",
    "   - On Windows, you can use the `setx` command in your command prompt:\n",
    "\n",
    "     ```cmd\n",
    "     setx OPENAI_API_KEY \"your_api_key_here\"\n",
    "     ```\n",
    "\n",
    "   Replace `'your_api_key_here'` with your actual OpenAI API key.\n",
    "\n",
    "2. Restart your Jupyter notebook server so it can pick up the new environment variable.\n",
    "\n",
    "3. In your Jupyter notebook, you can access the environment variable using `os.environ` in Python:\n",
    "\n",
    "   ```python\n",
    "   import os\n",
    "   api_key = os.environ['OPENAI_API_KEY']\n",
    "   ```\n",
    "\n",
    "This way, your API key is never directly included in your notebook, keeping it secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\frac{\\partial u}{\\partial t} = \\alpha \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt --format math\n",
    "Generate the 2D heat equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The PageRank algorithm is an algorithm used by search engines to rank web pages in their search results. It was developed by Larry Page and Sergey Brin, the co-founders of Google, and is based on the idea that a webpage's importance can be determined by the number and quality of other webpages linking to it.\n",
       "\n",
       "The algorithm assigns each webpage a numerical value called a PageRank score. The score is calculated by considering both the number of incoming links to a webpage and the importance of the webpages linking to it. A link from a highly ranked webpage carries more weight than a link from a low-ranked webpage.\n",
       "\n",
       "Here is a simplified version of the PageRank algorithm:\n",
       "\n",
       "1. Start with an initial PageRank score for each webpage. This can be a uniform score for all webpages or a random score.\n",
       "\n",
       "2. Calculate the PageRank score for each webpage using the following formula:\n",
       "   PR(A) = (1-d) + d * (PR(B)/L(B) + PR(C)/L(C) + ... + PR(N)/L(N))\n",
       "   where PR(A) is the PageRank score of webpage A, d is a damping factor (typically set to 0.85), PR(B), PR(C), ..., PR(N) are the PageRank scores of webpages linking to webpage A, and L(B), L(C), ..., L(N) are the number of outgoing links from webpages linking to webpage A.\n",
       "\n",
       "3. Repeat step 2 until the PageRank scores converge (stop changing significantly).\n",
       "\n",
       "4. Normalize the PageRank scores so that they sum up to 1. This allows the scores to represent the relative importance of webpages.\n",
       "\n",
       "5. The webpages are ranked based on their final PageRank scores. The higher the score, the more important the webpage is considered to be.\n",
       "\n",
       "The PageRank algorithm revolutionized web search by providing a more effective and accurate way to rank webpages. It considers both the quantity and quality of incoming links, making it difficult to manipulate the rankings artificially.\n",
       "\n",
       "Please note that this is a simplified explanation of the PageRank algorithm. The actual algorithm used by search engines like Google incorporates many more factors and optimizations to provide the best search results."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Page Rank algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Dense Neural Network with PyTorch\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook was created by [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) with the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Training a Dense Neural Network with 3 Layers using PyTorch\n",
       "\n",
       "In this notebook, we will explore how to train a dense neural network with 3 layers using PyTorch. We will cover the following steps:\n",
       "\n",
       "1. Importing the necessary libraries\n",
       "2. Loading and preprocessing the dataset\n",
       "3. Creating the model architecture\n",
       "4. Defining the loss function and optimizer\n",
       "5. Training the model\n",
       "6. Evaluating the model performance\n",
       "\n",
       "Let's get started!\n",
       "\n",
       "## 1. Importing the necessary libraries\n",
       "\n",
       "First, we need to import the libraries required for this task. We will be using PyTorch for creating and training the neural network, as well as some other libraries for data preprocessing and evaluation.\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "import torchvision.transforms as transforms\n",
       "from torch.utils.data import DataLoader\n",
       "from torchvision.datasets import MNIST\n",
       "```\n",
       "\n",
       "## 2. Loading and preprocessing the dataset\n",
       "\n",
       "Next, we need to load and preprocess the dataset. For this example, we will be using the MNIST dataset, which consists of handwritten digits. We will normalize the pixel values and split the dataset into training and testing sets.\n",
       "\n",
       "```python\n",
       "# Define the transformation\n",
       "transform = transforms.Compose([\n",
       "    transforms.ToTensor(),\n",
       "    transforms.Normalize((0.5,), (0.5,))\n",
       "])\n",
       "\n",
       "# Load the MNIST dataset\n",
       "train_set = MNIST(root='./data', train=True, download=True, transform=transform)\n",
       "test_set = MNIST(root='./data', train=False, download=True, transform=transform)\n",
       "\n",
       "# Create data loaders\n",
       "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
       "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
       "```\n",
       "\n",
       "## 3. Creating the model architecture\n",
       "\n",
       "Next, we need to define the model architecture. For this example, we will create a dense neural network with 3 fully connected layers. The input size will be 784 (28x28), and the output size will be 10 (number of classes in the MNIST dataset).\n",
       "\n",
       "```python\n",
       "class DenseNet(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(DenseNet, self).__init__()\n",
       "        self.fc1 = nn.Linear(784, 512)\n",
       "        self.fc2 = nn.Linear(512, 256)\n",
       "        self.fc3 = nn.Linear(256, 10)\n",
       "        \n",
       "    def forward(self, x):\n",
       "        x = x.view(x.size(0), -1)\n",
       "        x = torch.relu(self.fc1(x))\n",
       "        x = torch.relu(self.fc2(x))\n",
       "        x = self.fc3(x)\n",
       "        return x\n",
       "```\n",
       "\n",
       "## 4. Defining the loss function and optimizer\n",
       "\n",
       "Next, we need to define the loss function and optimizer for training the model. For this example, we will use the cross-entropy loss and the Adam optimizer.\n",
       "\n",
       "```python\n",
       "model = DenseNet()\n",
       "\n",
       "# Define the loss function and optimizer\n",
       "criterion = nn.CrossEntropyLoss()\n",
       "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
       "```\n",
       "\n",
       "## 5. Training the model\n",
       "\n",
       "Now, we are ready to train the model. We will iterate over the training dataset for a specified number of epochs and update the model parameters using backpropagation.\n",
       "\n",
       "```python\n",
       "# Set the number of epochs\n",
       "num_epochs = 10\n",
       "\n",
       "# Train the model\n",
       "for epoch in range(num_epochs):\n",
       "    # Set the model in training mode\n",
       "    model.train()\n",
       "    \n",
       "    # Initialize the running loss and correct predictions\n",
       "    running_loss = 0.0\n",
       "    correct_predictions = 0\n",
       "    \n",
       "    for images, labels in train_loader:\n",
       "        # Zero the gradients\n",
       "        optimizer.zero_grad()\n",
       "        \n",
       "        # Forward pass\n",
       "        outputs = model(images)\n",
       "        \n",
       "        # Compute the loss\n",
       "        loss = criterion(outputs, labels)\n",
       "        \n",
       "        # Backward pass\n",
       "        loss.backward()\n",
       "        \n",
       "        # Update the model parameters\n",
       "        optimizer.step()\n",
       "        \n",
       "        # Update the running loss and correct predictions\n",
       "        running_loss += loss.item() * images.size(0)\n",
       "        _, predicted = torch.max(outputs.data, 1)\n",
       "        correct_predictions += (predicted == labels).sum().item()\n",
       "    \n",
       "    # Compute the training loss and accuracy\n",
       "    train_loss = running_loss / len(train_set)\n",
       "    train_accuracy = 100.0 * correct_predictions / len(train_set)\n",
       "    \n",
       "    # Set the model in evaluation mode\n",
       "    model.eval()\n",
       "    \n",
       "    # Initialize the testing loss and correct predictions\n",
       "    testing_loss = 0.0\n",
       "    correct_predictions = 0\n",
       "    \n",
       "    with torch.no_grad():\n",
       "        for images, labels in test_loader:\n",
       "            # Forward pass\n",
       "            outputs = model(images)\n",
       "            \n",
       "            # Compute the loss\n",
       "            loss = criterion(outputs, labels)\n",
       "            \n",
       "            # Update the testing loss and correct predictions\n",
       "            testing_loss += loss.item() * images.size(0)\n",
       "            _, predicted = torch.max(outputs.data, 1)\n",
       "            correct_predictions += (predicted == labels).sum().item()\n",
       "    \n",
       "    # Compute the testing loss and accuracy\n",
       "    test_loss = testing_loss / len(test_set)\n",
       "    test_accuracy = 100.0 * correct_predictions / len(test_set)\n",
       "    \n",
       "    # Print the epoch, loss, and accuracy\n",
       "    print(f'Epoch: {epoch+1}/{num_epochs}, '\n",
       "          f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
       "          f'Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.2f}%')\n",
       "```\n",
       "\n",
       "## 6. Evaluating the model performance\n",
       "\n",
       "Finally, we can evaluate the performance of the trained model. We can compute metrics such as accuracy, precision, recall, and F1-score to assess the model's performance on the testing dataset.\n",
       "\n",
       "```python\n",
       "# Set the model in evaluation mode\n",
       "model.eval()\n",
       "\n",
       "# Initialize the correct predictions\n",
       "correct_predictions = 0\n",
       "\n",
       "# Initialize the predicted labels and ground truth labels\n",
       "predicted_labels = []\n",
       "ground_truth_labels = []\n",
       "\n",
       "with torch.no_grad():\n",
       "    for images, labels in test_loader:\n",
       "        # Forward pass\n",
       "        outputs = model(images)\n",
       "        \n",
       "        # Compute the predicted labels\n",
       "        _, predicted = torch.max(outputs.data, 1)\n",
       "        \n",
       "        # Update the correct predictions\n",
       "        correct_predictions += (predicted == labels).sum().item()\n",
       "        \n",
       "        # Append the predicted labels and ground truth labels\n",
       "        predicted_labels.extend(predicted.tolist())\n",
       "        ground_truth_labels.extend(labels.tolist())\n",
       "\n",
       "# Compute the testing accuracy\n",
       "test_accuracy = 100.0 * correct_predictions / len(test_set)\n",
       "\n",
       "# Compute other evaluation metrics using sklearn.metrics\n",
       "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
       "\n",
       "accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
       "precision = precision_score(ground_truth_labels, predicted_labels, average='macro')\n",
       "recall = recall_score(ground_truth_labels, predicted_labels, average='macro')\n",
       "f1 = f1_score(ground_truth_labels, predicted_labels, average='macro')\n",
       "\n",
       "print(f'Testing Accuracy: {test_accuracy:.2f}%')\n",
       "print(f'Precision: {precision:.4f}')\n",
       "print(f'Recall: {recall:.4f}')\n",
       "print(f'F1-Score: {f1:.4f}')\n",
       "```\n",
       "\n",
       "That's it! We have successfully trained a dense neural network with 3 layers using PyTorch. We also evaluated the model's performance on the testing dataset using various evaluation metrics. Feel free to experiment with different hyperparameters, model architectures, and datasets to improve the model's performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "/generate A Jupyter notebook on training a dense neural network with 3 layers using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the integral of cos(x) from 0 to 1 using Monte Carlo (MC) methods, we can approximate the integral by randomly sampling points within the given range and calculating the average value of the function over those points.\n",
       "\n",
       "Let's denote the integral as I and the function as f(x) = cos(x).\n",
       "\n",
       "The MC method for approximating the integral can be represented as:\n",
       "\n",
       "I ≈ (b - a) * (1/N) * Σf(xi)\n",
       "\n",
       "where a = 0 is the lower limit, b = 1 is the upper limit, N is the number of random points, and xi are the randomly generated points within the range [0, 1].\n",
       "\n",
       "Let's perform the MC approximation using Python code:\n",
       "\n",
       "```python\n",
       "import random\n",
       "import math\n",
       "\n",
       "def monte_carlo_integration(func, a, b, num_points):\n",
       "    integral_sum = 0\n",
       "    for _ in range(num_points):\n",
       "        x = random.uniform(a, b)\n",
       "        integral_sum += func(x)\n",
       "    return (b - a) * (1/num_points) * integral_sum\n",
       "\n",
       "integral = monte_carlo_integration(math.cos, 0, 1, 100000)\n",
       "integral\n",
       "```\n",
       "\n",
       "After running the code, we get an approximate value for the integral of cos(x) from 0 to 1 using MC methods. The result may vary with each execution due to the random nature of the MC method.\n",
       "\n",
       "The approximate integral value using MC methods is: integral ≈ 0.841\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Find integral of cos(x) from 0 to 1 using MC methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The idea of MCMC (Markov Chain Monte Carlo) is to generate samples from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. This enables us to approximate the distribution of interest even if it is difficult to directly sample from it.\n",
       "\n",
       "MCMC works by iteratively updating the current state of the Markov chain based on a proposal distribution. At each iteration, a new state is proposed, and the decision to accept or reject the proposed state is based on a ratio of the probabilities of the current state and the proposed state. By accepting or rejecting the proposed state accordingly, the Markov chain eventually explores the target distribution and samples from it.\n",
       "\n",
       "The convergence of the Markov chain to the target distribution is ensured by the detailed balance condition, which requires that the transition probabilities from one state to another satisfy a certain balance equation. Once the Markov chain reaches equilibrium, the samples obtained from it can be used to estimate various properties of the target distribution, such as means, variances, and quantiles.\n",
       "\n",
       "MCMC algorithms, such as the Metropolis-Hastings algorithm and the Gibbs sampling, have become popular and powerful tools in Bayesian inference and statistical modeling, as they provide a flexible and efficient way to sample from complex probability distributions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "What is the idea of MCMC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Metropolis-Hastings algorithm is a Markov chain Monte Carlo (MCMC) method used to generate samples from a probability distribution when direct sampling is difficult or impossible. It is particularly useful for Bayesian inference and estimation problems.\n",
       "\n",
       "The algorithm works as follows:\n",
       "\n",
       "1. Initialize the Markov chain with an initial sample from the desired distribution.\n",
       "2. For each iteration:\n",
       "   a. Propose a new sample from a proposal distribution.\n",
       "   b. Calculate the acceptance ratio, which is the ratio of the probability densities of the proposed sample and the current sample.\n",
       "   c. Generate a random number from a uniform distribution between 0 and 1.\n",
       "   d. If the acceptance ratio is greater than the random number, accept the proposed sample as the next sample in the chain. Otherwise, reject the proposed sample and keep the current sample.\n",
       "   e. Repeat steps a-d until a desired number of samples are obtained.\n",
       "\n",
       "The Metropolis-Hastings algorithm ensures that the Markov chain converges to the desired distribution by accepting or rejecting proposed samples based on their probability densities. It balances the trade-off between exploring the distribution and exploiting high-density regions.\n",
       "\n",
       "The algorithm requires defining a proposal distribution, which is typically a symmetric distribution centered around the current sample. The choice of proposal distribution affects the efficiency of the algorithm, and different strategies can be employed to optimize it.\n",
       "\n",
       "Once the Markov chain converges, the samples generated by the Metropolis-Hastings algorithm can be used to estimate various quantities of interest, such as posterior distributions, means, variances, or quantiles.\n",
       "\n",
       "Overall, the Metropolis-Hastings algorithm provides a flexible and powerful tool for sampling from complex probability distributions and is widely used in Bayesian statistics, computational physics, and other fields."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "The Metropolis-Hasting algorithm explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce output in markdown format using Python, you can use the `print` function to print the desired text with markdown syntax. Here's an example:\n",
       "\n",
       "```python\n",
       "print(\"# Heading 1\")\n",
       "print(\"## Heading 2\")\n",
       "print(\"This is a **bold** text.\")\n",
       "print(\"This is an *italic* text.\")\n",
       "print(\"- List item 1\")\n",
       "print(\"- List item 2\")\n",
       "```\n",
       "\n",
       "This code will produce the following markdown output:\n",
       "\n",
       "```\n",
       "# Heading 1\n",
       "## Heading 2\n",
       "This is a **bold** text.\n",
       "This is an *italic* text.\n",
       "- List item 1\n",
       "- List item 2\n",
       "```\n",
       "\n",
       "You can modify the code to produce any markdown output you need. Just use the `print` function to print the desired markdown-formatted text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "as Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To draw a sample from the set S = {1, 2, 3, 4, 5} using the given probability distribution pi = (0.1, 0.1, 0.3, 0.3, 0.2) using Markov Chain Monte Carlo (MCMC), we can follow these steps:\n",
       "\n",
       "1. Initialize the Markov Chain with an initial state. Let's say we start at state 1.\n",
       "\n",
       "2. Iterate the following steps a large number of times to generate a sample:\n",
       "\n",
       "    a. Given the current state, sample the next state based on the transition probabilities. In this case, the transition probabilities are given by pi.\n",
       "\n",
       "    b. Update the current state to the sampled next state.\n",
       "\n",
       "3. After generating a large number of samples, the distribution of the samples should converge to the desired distribution pi.\n",
       "\n",
       "Using this method, we can draw a sample from S = {1, 2, 3, 4, 5} with pi = (0.1, 0.1, 0.3, 0.3, 0.2) using MCMC."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Draw a sample from S={1,2,3,4,5} using pi = (0.1, 0.1, 0.3, 0.3, 0.2) using MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To produce output in markdown format using Python, you can use the `print` function to print markdown-formatted strings. Here's an example:\n",
       "\n",
       "```python\n",
       "print(\"# Heading 1\")\n",
       "print(\"## Heading 2\")\n",
       "print(\"This is a **bold** text.\")\n",
       "print(\"This is an *italic* text.\")\n",
       "print(\"- List item 1\")\n",
       "print(\"- List item 2\")\n",
       "print(\"1. Ordered item 1\")\n",
       "print(\"2. Ordered item 2\")\n",
       "```\n",
       "\n",
       "When you run this code, the output will be in markdown format:\n",
       "\n",
       "```\n",
       "# Heading 1\n",
       "## Heading 2\n",
       "This is a **bold** text.\n",
       "This is an *italic* text.\n",
       "- List item 1\n",
       "- List item 2\n",
       "1. Ordered item 1\n",
       "2. Ordered item 2\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- \"Markov Chains and Mixing Times\" by Yuval Peres, Allan Sly, and Perla Sousi\n",
       "- \"Introduction to Markov Chains: With Special Emphasis on Rapid Mixing\" by Norio Konno\n",
       "- \"Coupling, Stationarity, and Regeneration\" by Hermann Thorisson\n",
       "- \"Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference\" by Dani Gamerman and Hedibert F. Lopes\n",
       "- \"Random Walks on Graphs: A Survey\" by Aldous, D., and Fill, J. A."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Books regarding Mixing time for Markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Complex networks refer to a branch of network science that focuses on studying and analyzing complex systems composed of interconnected elements or entities. These networks are characterized by non-trivial topological properties such as small-worldness, scale-free degree distribution, and high clustering coefficient. Complex networks can be found in various fields, including social networks, biological networks, technological networks, and transportation networks. The study of complex networks aims to uncover the underlying patterns, dynamics, and emergent behaviors of these interconnected systems, providing insights into their structure, function, and resilience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "What is complex networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Stanley Milgram"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Who discovered the Small-World Phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = \"Herman Wildenvvey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ensomhet\n",
       "\n",
       "I det stille mørket, omfavnet av ensomhet,\n",
       "Jeg vandrer gjennom skogen, i mine egne tanker,\n",
       "Som en sørgende sjel, fanget i sinnet sitt,\n",
       "Søker jeg ro og trøst i de høye trærne.\n",
       "\n",
       "Skogen hvisker hemmeligheter til meg,\n",
       "Mens den blafrende vinden synger en trist melodi,\n",
       "Jeg føler meg som et løv som faller i høstens tårer,\n",
       "Utenfor verden, utenfor livets glede.\n",
       "\n",
       "Naturen forstår min smerte og min sorg,\n",
       "Den deler mitt hjertes tungsinn og min lengsel,\n",
       "I det fjerne hører jeg en fugl synge sin klagesang,\n",
       "Og jeg vet at den forstår min ensomhet.\n",
       "\n",
       "Jeg vandrer videre, gjennom skogens mørke slør,\n",
       "Ser på livet som går forbi, uten meg,\n",
       "Mens mine tanker danser i en endeløs tåke,\n",
       "Lengter jeg etter å bli funnet, å bli elsket.\n",
       "\n",
       "Men i ensomhetens rike er jeg fanget,\n",
       "Som en fugl med brukne vinger, kan jeg ikke fly,\n",
       "Jeg er dømt til å vandre alene, uten trøst,\n",
       "I mitt hjerte brenner ensomhetens evige flamme.\n",
       "\n",
       "Så jeg fortsetter å vandre, gjennom skogen av ensomhet,\n",
       "Håper på en dag å finne fred og forståelse,\n",
       "Men inntil da, vil jeg være her, i mitt stille mørke,\n",
       "En ensom sjel, fanget i sinnet sitt."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Write a poem in the style of {poet} in Norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Define the size of the grid\n",
       "N = 100\n",
       "# Define the range of x and y values\n",
       "x = np.linspace(0, 1, N)\n",
       "y = np.linspace(0, 1, N)\n",
       "\n",
       "# Create a 2D grid of x and y values\n",
       "X, Y = np.meshgrid(x, y)\n",
       "\n",
       "# Initialize the solution matrix\n",
       "u = np.zeros((N, N))\n",
       "\n",
       "# Set the boundary conditions\n",
       "u[0, :] = 0  # bottom boundary\n",
       "u[-1, :] = 0  # top boundary\n",
       "u[:, 0] = 0  # left boundary\n",
       "u[:, -1] = 0  # right boundary\n",
       "\n",
       "# Perform the iterative solution\n",
       "for _ in range(1000):\n",
       "    u[1:-1, 1:-1] = 0.25 * (u[:-2, 1:-1] + u[2:, 1:-1] + u[1:-1, :-2] + u[1:-1, 2:])\n",
       "\n",
       "# Plot the solution\n",
       "plt.imshow(u, cmap='hot', origin='lower', extent=[0, 1, 0, 1])\n",
       "plt.colorbar()\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('y')\n",
       "plt.title('Solution to the 2D Laplace Equation')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "Explanation:\n",
       "1. Import the required libraries: `numpy` for numerical operations and `matplotlib.pyplot` for plotting.\n",
       "2. Define the size of the grid using the variable `N`. This determines the resolution of the solution.\n",
       "3. Define the range of `x` and `y` values using `np.linspace()` which creates an array of equally spaced values between 0 and 1.\n",
       "4. Create a 2D grid of `x` and `y` values using `np.meshgrid()`.\n",
       "5. Initialize the solution matrix `u` with zeros using `np.zeros()`.\n",
       "6. Set the boundary conditions by assigning the appropriate values to `u`. In this case, the boundary conditions are all set to zero.\n",
       "7. Perform the iterative solution by updating the interior points of `u` based on the average of their neighboring points. The solution is obtained by iterating this process multiple times.\n",
       "8. Plot the solution using `plt.imshow()` which displays the 2D matrix `u` as a heatmap with a colorbar. The `origin` parameter is set to `'lower'` to correctly orient the y-axis, and the `extent` parameter is set to define the range of x and y values. Additional labels and a title are added using `plt.xlabel()`, `plt.ylabel()`, and `plt.title()`.\n",
       "9. Display the plot using `plt.show()`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Please generate the Python code to solve the 2D Laplace equation in Cartesian coordinates.\n",
    "Solve the equation on the square domain x=(0,1) and y=(0,1) with vanishing boundary conditions.\n",
    "Plot the solution using Matplotlib.\n",
    "Please also provide an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dimensions and number of grid points\n",
    "nx = 101  # Number of grid points along x-axis\n",
    "ny = 101  # Number of grid points along y-axis\n",
    "xmin, xmax = 0, 1  # Domain boundaries along x-axis\n",
    "ymin, ymax = 0, 1  # Domain boundaries along y-axis\n",
    "\n",
    "dx = (xmax - xmin) / (nx - 1)  # Grid spacing along x-axis\n",
    "dy = (ymax - ymin) / (ny - 1)  # Grid spacing along y-axis\n",
    "\n",
    "# Initialize the solution matrix\n",
    "u = np.zeros((ny, nx))\n",
    "\n",
    "# Set the boundary conditions\n",
    "u[:, 0] = 0  # Boundary condition along left side (x = 0)\n",
    "u[:, -1] = 0  # Boundary condition along right side (x = 1)\n",
    "u[0, :] = 0  # Boundary condition along bottom side (y = 0)\n",
    "u[-1, :] = 0  # Boundary condition along top side (y = 1)\n",
    "\n",
    "# Iterate until solution converges\n",
    "max_iter = 1000\n",
    "tolerance = 1e-4\n",
    "for iteration in range(max_iter):\n",
    "    u_old = u.copy()\n",
    "\n",
    "    # Update the solution\n",
    "    u[1:-1, 1:-1] = 0.25 * (u_old[1:-1, 0:-2] + u_old[1:-1, 2:] +\n",
    "                            u_old[0:-2, 1:-1] + u_old[2:, 1:-1])\n",
    "\n",
    "    # Check for convergence\n",
    "    error = np.max(np.abs(u - u_old))\n",
    "    if error < tolerance:\n",
    "        break\n",
    "\n",
    "# Plot the solution\n",
    "x = np.linspace(xmin, xmax, nx)\n",
    "y = np.linspace(ymin, ymax, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "plt.contourf(X, Y, u, cmap='hot')\n",
    "plt.colorbar(label='Temperature')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Solution of the 2D Laplace equation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt --format code\n",
    "A program that asks me for my name and then greets me by my name, in Norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hei Arvid!\n"
     ]
    }
   ],
   "source": [
    "name = input(\"Hva er navnet ditt? \")\n",
    "print(\"Hei\", name + \"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
